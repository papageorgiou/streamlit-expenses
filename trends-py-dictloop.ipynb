{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7067766,"sourceType":"datasetVersion","datasetId":4069880}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytrends","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-03T07:22:32.200954Z","iopub.execute_input":"2023-12-03T07:22:32.202009Z","iopub.status.idle":"2023-12-03T07:22:48.933206Z","shell.execute_reply.started":"2023-12-03T07:22:32.201968Z","shell.execute_reply":"2023-12-03T07:22:48.931900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-12-03T07:24:31.967796Z","iopub.execute_input":"2023-12-03T07:24:31.968262Z","iopub.status.idle":"2023-12-03T07:24:32.323047Z","shell.execute_reply.started":"2023-12-03T07:24:31.968226Z","shell.execute_reply":"2023-12-03T07:24:32.321546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytrends.request import TrendReq\nimport pandas as pd\nimport logging\nfrom datetime import datetime\n\nfrom itertools import product\nfrom tenacity import retry, stop_after_attempt, wait_exponential, after_log\n\n# Configure logging\nlogging.basicConfig(datefmt='%d-%m-%Y:%H:%M:%S', filename=\"pytrendslog.log\", filemode='w',\n                    level=logging.INFO, format='%(levelname)s - %(message)s')\nlogger = logging.getLogger('pytrendslogger')\n\ncats_df = pd.read_csv(\"/kaggle/input/categories/trendscategories_distinct.csv\")\n\n\n# Initialize pytrends\npytrends = TrendReq(hl='en-US', tz=360, retries=0, backoff_factor=0)\n\n# Define the retry decorator with logging\nretry_deco = retry(reraise=True, stop=stop_after_attempt(8),\n                        wait=wait_exponential(multiplier=1, min=2, max=1800),\n                        after=after_log(logger, logging.INFO))\n\n\n\n  \n\nkeyword = \"\"\ncat = cats_df['id']\nstart_date='2018-01-01'\nend_date = '2023-10-30'\ngeo= ['US', \"GB\"]\n\ncross_product = (pd.DataFrame(product(cat, geo), columns=['cat', 'geo'] )\n                .assign(keyword=keyword,start_date=start_date, end_date=end_date))\n\n\ncat_arg_df = (cross_product\n              .query('geo == \"US\"')\n              .reset_index(drop=True)\n              .assign(iteration=lambda x: range(1, len(x) + 1),\n                      total=lambda x: len(x)))\n                      \n\n\n# alternative with dic directly...\n\n# keyword = \"\"\n# start_date = '2018-01-01'\n# end_date = '2023-10-30'\n# geo = ['US', \"GB\"]\n# cat_arg_dicts = [\n#     {'keyword': keyword, 'cat': cat_id, 'start_date': start_date, 'end_date': end_date, 'geo': g}\n#     for cat_id, g in product(cats_df['id'], geo)]\n# \n\n\ncat_arg_dicts = cat_arg_df.to_dict(orient=\"records\")\n\nlen(cat_arg_dicts)\ncat_arg_dicts[1]\n\n\n\n\n\n####  Enter function HERE  ########################\n\n\n@retry_deco\ndef get_interestovertime(keyword=\"\", cat=0, start_date='2018-01-01', end_date='2023-05-30', geo='', gprop='', iteration=0, total=0, **kwargs):\n    pytrends.build_payload(kw_list=[keyword], cat=cat, timeframe=f\"{start_date} {end_date}\", geo=geo, gprop=gprop)\n    \n    iot_df = pytrends.interest_over_time()\n    \n    if iot_df.empty:\n        pass\n        # data = [[datetime.strptime(start_date, '%Y-%m-%d'), 0, keyword, cat]] # datetime.strptime(start_date_str, '%Y-%m-%d').date()\n        # columns=['date', 'value', 'query', 'category_df']\n        # iot_df = pd.DataFrame(data, columns=columns)  # Ensure the same structure as the expected dataframe\n    else:\n        iot_df = (\n            iot_df.reset_index()\n            .rename(columns={keyword: \"value\"})\n            .assign(query=keyword, category_df=cat)\n            #.assign(date=lambda x: x['date'].dt.date) # iot['date'] = iot['date'].dt.date REMOVE hour component \n        )\n\n        if 'isPartial' in iot_df.columns:\n            iot_df = iot_df.drop(columns='isPartial')\n        \n        if 'index' in iot_df.columns:\n            iot_df = iot_df.drop(columns='index')\n\n    logger.info(f'\\n logger: got IOT {len(iot_df)} rows .....for seed term : \"{keyword}\" and category: \"{cat}\" ......\\n progress: {iteration} of  {total}\\n') \n\n    return iot_df\n\n\n# For get_top_rising  (it either gets top and  rising or nothing at all...to keep it simple!)\n# WORKS\n@retry_deco\ndef get_top_rising(keyword=\"\", cat=0, start_date='2018-01-01', end_date='2023-05-30', geo='', gprop='', iteration=0, total=0, **kwargs): \n    logger.info(f'\\n logger: fetching top & rising for seed term : \"{keyword}\" and category: \"{cat}\"......\\n') \n    \n    pytrends.build_payload(kw_list=[keyword], cat=cat, timeframe=f\"{start_date} {end_date}\", geo=geo, gprop=gprop)\n    \n    top_rising_dict = pytrends.related_queries()\n\n    topdf = top_rising_dict[keyword][\"top\"]\n    risingdf = top_rising_dict[keyword][\"rising\"]\n\n    \n    if topdf is not None and risingdf is not None:\n        topdf = topdf.assign(type=\"top\")\n        risingdf = risingdf.assign(type=\"rising\")  \n        top_rising_df = pd.concat([topdf, risingdf], axis=0, ignore_index=True).assign(category_df=cat) # -not needed in case of pmap /otherwise we'll need cat + keyword\n\n    else:   # then surely rising df is also none...\n        #top_rising_df = pd.DataFrame(columns=['query', 'value', 'type', 'category_df'])\n        top_rising_df = pd.DataFrame()\n\n    # else:\n    #     top_rising_df = topdf.assign(type=\"top\")\n            \n    logger.info(f'\\n logger: got top_rising {len(top_rising_df)} rows .....for seed term : \"{keyword}\" and category: \"{cat}\" ......\\n progress: {iteration} of  {total}\\n') \n    \n    return top_rising_df\n \n\n\n\n\n\n\nimport traceback\n\n# Initialize an empty list to hold summary information\nsummary_info = []\nresults= []\n\nfor i, dictio in enumerate(cat_arg_dicts):\n    # Initialize a dictionary to store the summary for this iteration\n    summary = dictio.copy()\n    try:\n        logger.info(f\"Starting iteration: {i}\")\n        result = get_top_rising(**dictio)\n        \n        # Add result info to summary\n        summary['Number of Results'] = len(result)\n        summary['Exception'] = ''\n        summary['Stack Trace'] = ''  # Initialize Stack Trace as empty\n        \n        # If result is not empty, append it to results list\n        if not result.empty:\n            results.append(result)\n        \n    except Exception as e:\n        # If an exception occurs, log it and store in the summary\n        error_message = str(e)\n        stack_trace = traceback.format_exc()  # Get the stack trace\n        logger.exception(f\"Exception for {dictio}: {error_message}\")\n        \n        # Update summary with exception details and indicate 0 results\n        summary['Number of Results'] = 0\n        summary['Exception'] = error_message\n        summary['Stack Trace'] = stack_trace  # Add the stack trace to the same row\n    \n    # Append the summary dictionary to the list\n    summary_info.append(summary)\n\n# Convert the list of dictionaries to a DataFrame\nsummary_df = pd.DataFrame(summary_info)\n\n# If you had results, concatenate them into a DataFrame\nif results:\n    df_response = pd.concat(results)\n    df_response.to_pickle(\"df_response_v3.pkl\")\n    df_response.to_csv(\"df_response_v3.csv\")\nelse:\n    # No results to save, but we still create an empty DataFrame with the appropriate columns\n    df_response = pd.DataFrame(columns=['query', 'value', 'type', 'category_df'])\n\n# Now you have `summary_df` with all the information and `df_response` with the concatenated results\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-03T07:25:19.375367Z","iopub.execute_input":"2023-12-03T07:25:19.375837Z","iopub.status.idle":"2023-12-03T07:25:22.745290Z","shell.execute_reply.started":"2023-12-03T07:25:19.375783Z","shell.execute_reply":"2023-12-03T07:25:22.744004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_df.to_pickle(\"df_summary_v3.pkl\")\nsummary_df.to_csv(\"df_summary_v3.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_response)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_response","metadata":{"execution":{"iopub.status.busy":"2023-12-03T07:25:41.736141Z","iopub.execute_input":"2023-12-03T07:25:41.736623Z","iopub.status.idle":"2023-12-03T07:25:41.762159Z","shell.execute_reply.started":"2023-12-03T07:25:41.736583Z","shell.execute_reply":"2023-12-03T07:25:41.760853Z"},"trusted":true},"execution_count":null,"outputs":[]}]}